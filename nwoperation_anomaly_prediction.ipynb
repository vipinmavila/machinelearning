{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module importing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports: All imports are added in the below cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Nov 21 15:37:26 2018\n"
     ]
    }
   ],
   "source": [
    "##imports\n",
    "import time\n",
    "print(time.asctime())\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import neighbors\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "from imblearn.under_sampling import EditedNearestNeighbours\n",
    "from sklearn import preprocessing\n",
    "import pickle\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constants: All constants are added in the below cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Nov 21 15:37:47 2018\n",
      "CPU times: user 154 µs, sys: 32 µs, total: 186 µs\n",
      "Wall time: 159 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print(time.asctime())\n",
    "\n",
    "##constants\n",
    "null_val_threshold = 90\n",
    "unique_threshold   = 1\n",
    "output_variable = 'isAnomaly' \n",
    "variance_threshold = 0.9*(1-0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading input data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the train_data.csv file to a pandas dataframe \"original_train_df\". Save the dataframe to pickle \"originaltrainpickle\". \n",
    "1) The shape of training dataframe is (750199, 237), i.e., 750199 samples and 237 features.\n",
    "2) The size of dataframe is 1.320GB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Nov 21 15:37:47 2018\n",
      "\n",
      "original train data shape (750199, 237)\n",
      "\n",
      "memory footprint of original train df = 1.320GB\n",
      "CPU times: user 28.3 s, sys: 5.33 s, total: 33.7 s\n",
      "Wall time: 46.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print(time.asctime())\n",
    "\n",
    "# Read data\n",
    "original_train_df = pd.read_csv(\"/home/fedora/programs/group3/Challenge_A/operational_data/train_data.csv\",\n",
    "                                sep=\",\",\n",
    "                                #lineterminator=\"\\n\",\n",
    "                                header=0)\n",
    "print(\"\\noriginal train data shape\", original_train_df.shape)\n",
    "print (\"\\nmemory footprint of original train df = %0.3fGB\" %(original_train_df.memory_usage(deep=True).sum()/1024**3))\n",
    "original_train_df.to_pickle(\"originaltrainpickle\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the data description\n",
    "What can be inferred from the below description?\n",
    "1) There seem to be many features with single unique values (mainly 0 values)\n",
    "2) The min and max values of different features shows high variance. It seems the different feature values are having different spread(std shows high spread)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Nov 21 15:38:33 2018\n",
      "          Unnamed: 0   Unnamed: 0.1           host        process  \\\n",
      "count  750199.000000  750199.000000  750199.000000  750199.000000   \n",
      "mean   375099.000000   19066.559100       4.329551       0.500103   \n",
      "std    216563.941634   11379.675984       2.847125       0.500000   \n",
      "min         0.000000       0.000000       0.000000       0.000000   \n",
      "25%    187549.500000    9370.000000       2.000000       0.000000   \n",
      "50%    375099.000000   18750.000000       4.000000       1.000000   \n",
      "75%    562648.500000   28140.000000       7.000000       1.000000   \n",
      "max    750198.000000   54030.000000       9.000000       1.000000   \n",
      "\n",
      "       Prepared statement cache hit rate : ((MXBean(com.bea:Name=source06,Type=JDBCDataSourceRuntime).PrepStmtCacheHitCount / MXBean(com.bea:Name=source06,Type=JDBCDataSourceRuntime).PrepStmtCacheMissCount))  \\\n",
      "count                                      750199.000000                                                                                                                                                          \n",
      "mean                                            1.015650                                                                                                                                                          \n",
      "std                                             0.186633                                                                                                                                                          \n",
      "min                                             0.005794                                                                                                                                                          \n",
      "25%                                             1.000000                                                                                                                                                          \n",
      "50%                                             1.000000                                                                                                                                                          \n",
      "75%                                             1.000000                                                                                                                                                          \n",
      "max                                             3.880492                                                                                                                                                          \n",
      "\n",
      "       Memory space usage : ((MXBean(java.lang:name=Code Cache,type=MemoryPool).Usage.committed / MXBean(java.lang:name=Code Cache,type=MemoryPool).Usage.max))  \\\n",
      "count                                      750199.000000                                                                                                          \n",
      "mean                                            0.739074                                                                                                          \n",
      "std                                             0.257227                                                                                                          \n",
      "min                                             0.053467                                                                                                          \n",
      "25%                                             0.541667                                                                                                          \n",
      "50%                                             0.841146                                                                                                          \n",
      "75%                                             0.970052                                                                                                          \n",
      "max                                             0.981771                                                                                                          \n",
      "\n",
      "       Active connections : (MXBean(com.bea:Name=source04,Type=JDBCConnectionPoolRuntime).ActiveConnectionsCurrentCount)  \\\n",
      "count                                      750199.000000                                                                   \n",
      "mean                                            0.000571                                                                   \n",
      "std                                             0.023879                                                                   \n",
      "min                                             0.000000                                                                   \n",
      "25%                                             0.000000                                                                   \n",
      "50%                                             0.000000                                                                   \n",
      "75%                                             0.000000                                                                   \n",
      "max                                             1.000000                                                                   \n",
      "\n",
      "       Available db connection activity : (d/dx (MXBean(com.bea:Name=source02,Type=JDBCDataSourceRuntime).NumAvailable))  \\\n",
      "count                                      750199.000000                                                                   \n",
      "mean                                           -0.000024                                                                   \n",
      "std                                             0.012647                                                                   \n",
      "min                                            -2.000000                                                                   \n",
      "25%                                             0.000000                                                                   \n",
      "50%                                             0.000000                                                                   \n",
      "75%                                             0.000000                                                                   \n",
      "max                                             1.000000                                                                   \n",
      "\n",
      "       Active connections : (MXBean(com.bea:Name=source03,Type=JDBCConnectionPoolRuntime).ActiveConnectionsCurrentCount)  \\\n",
      "count                                      750199.000000                                                                   \n",
      "mean                                            0.001813                                                                   \n",
      "std                                             0.042570                                                                   \n",
      "min                                             0.000000                                                                   \n",
      "25%                                             0.000000                                                                   \n",
      "50%                                             0.000000                                                                   \n",
      "75%                                             0.000000                                                                   \n",
      "max                                             2.000000                                                                   \n",
      "\n",
      "       DB connection started : (incld/dx (MXBean(com.bea:Name=source02,Type=JDBCDataSourceRuntime).ConnectionsTotalCount))  \\\n",
      "count                                      750199.000000                                                                     \n",
      "mean                                            0.000027                                                                     \n",
      "std                                             0.005163                                                                     \n",
      "min                                             0.000000                                                                     \n",
      "25%                                             0.000000                                                                     \n",
      "50%                                             0.000000                                                                     \n",
      "75%                                             0.000000                                                                     \n",
      "max                                             1.000000                                                                     \n",
      "\n",
      "              ...          \\\n",
      "count         ...           \n",
      "mean          ...           \n",
      "std           ...           \n",
      "min           ...           \n",
      "25%           ...           \n",
      "50%           ...           \n",
      "75%           ...           \n",
      "max           ...           \n",
      "\n",
      "       Available db connection activity : (d/dx (MXBean(com.bea:Name=source10,Type=JDBCConnectionPoolRuntime).NumAvailable))  \\\n",
      "count                                      750199.000000                                                                       \n",
      "mean                                            0.000001                                                                       \n",
      "std                                             0.002000                                                                       \n",
      "min                                            -1.000000                                                                       \n",
      "25%                                             0.000000                                                                       \n",
      "50%                                             0.000000                                                                       \n",
      "75%                                             0.000000                                                                       \n",
      "max                                             1.000000                                                                       \n",
      "\n",
      "       Rel. unavailable connections : ((MXBean(com.bea:Name=source04,Type=JDBCDataSourceRuntime).NumUnavailable / MXBean(com.bea:Name=source04,Type=JDBCDataSourceRuntime).CurrCapacity))  \\\n",
      "count                                      750199.000000                                                                                                                                    \n",
      "mean                                            0.000783                                                                                                                                    \n",
      "std                                             0.027940                                                                                                                                    \n",
      "min                                             0.000000                                                                                                                                    \n",
      "25%                                             0.000000                                                                                                                                    \n",
      "50%                                             0.000000                                                                                                                                    \n",
      "75%                                             0.000000                                                                                                                                    \n",
      "max                                             1.000000                                                                                                                                    \n",
      "\n",
      "       Failed wait for connection : (incld/dx (MXBean(com.bea:Name=source08,Type=JDBCDataSourceRuntime).WaitingForConnectionFailureTotal))  \\\n",
      "count                                           750199.0                                                                                     \n",
      "mean                                                 0.0                                                                                     \n",
      "std                                                  0.0                                                                                     \n",
      "min                                                  0.0                                                                                     \n",
      "25%                                                  0.0                                                                                     \n",
      "50%                                                  0.0                                                                                     \n",
      "75%                                                  0.0                                                                                     \n",
      "max                                                  0.0                                                                                     \n",
      "\n",
      "       Rel. unavailable connections : ((MXBean(com.bea:Name=source08,Type=JDBCDataSourceRuntime).NumUnavailable / MXBean(com.bea:Name=source08,Type=JDBCDataSourceRuntime).CurrCapacity))  \\\n",
      "count                                       750199.00000                                                                                                                                    \n",
      "mean                                             0.99989                                                                                                                                    \n",
      "std                                              0.01047                                                                                                                                    \n",
      "min                                              0.00000                                                                                                                                    \n",
      "25%                                              1.00000                                                                                                                                    \n",
      "50%                                              1.00000                                                                                                                                    \n",
      "75%                                              1.00000                                                                                                                                    \n",
      "max                                              1.00000                                                                                                                                    \n",
      "\n",
      "       Stuck threads : (MXBean(com.bea:ApplicationRuntime=source05,Name=default,Type=WorkManagerRuntime).StuckThreadCount)  \\\n",
      "count                                           750199.0                                                                     \n",
      "mean                                                 0.0                                                                     \n",
      "std                                                  0.0                                                                     \n",
      "min                                                  0.0                                                                     \n",
      "25%                                                  0.0                                                                     \n",
      "50%                                                  0.0                                                                     \n",
      "75%                                                  0.0                                                                     \n",
      "max                                                  0.0                                                                     \n",
      "\n",
      "       Process CPU : (\\Process(java)\\CPU)  \\\n",
      "count                       750199.000000   \n",
      "mean                            21.528531   \n",
      "std                             74.601941   \n",
      "min                              0.000000   \n",
      "25%                              0.000000   \n",
      "50%                              0.000000   \n",
      "75%                              2.000000   \n",
      "max                           1556.000000   \n",
      "\n",
      "       Successful wait for connection : (incld/dx (MXBean(com.bea:Name=source03,Type=JDBCDataSourceRuntime).WaitingForConnectionSuccessTotal))  \\\n",
      "count                                           750199.0                                                                                         \n",
      "mean                                                 0.0                                                                                         \n",
      "std                                                  0.0                                                                                         \n",
      "min                                                  0.0                                                                                         \n",
      "25%                                                  0.0                                                                                         \n",
      "50%                                                  0.0                                                                                         \n",
      "75%                                                  0.0                                                                                         \n",
      "max                                                  0.0                                                                                         \n",
      "\n",
      "       Failed wait for connection : (incld/dx (MXBean(com.bea:Name=source03,Type=JDBCDataSourceRuntime).WaitingForConnectionFailureTotal))  \\\n",
      "count                                           750199.0                                                                                     \n",
      "mean                                                 0.0                                                                                     \n",
      "std                                                  0.0                                                                                     \n",
      "min                                                  0.0                                                                                     \n",
      "25%                                                  0.0                                                                                     \n",
      "50%                                                  0.0                                                                                     \n",
      "75%                                                  0.0                                                                                     \n",
      "max                                                  0.0                                                                                     \n",
      "\n",
      "       Connection delay : (MXBean(com.bea:Name=source02,Type=JDBCDataSourceRuntime).ConnectionDelayTime)  \\\n",
      "count                                      750199.000000                                                   \n",
      "mean                                           99.753173                                                   \n",
      "std                                           212.284152                                                   \n",
      "min                                            17.716360                                                   \n",
      "25%                                            75.000000                                                   \n",
      "50%                                            84.000000                                                   \n",
      "75%                                            88.000000                                                   \n",
      "max                                          3751.000000                                                   \n",
      "\n",
      "       seconds_from_start  \n",
      "count        7.501990e+05  \n",
      "mean         1.298525e+07  \n",
      "std          8.193130e+06  \n",
      "min          0.000000e+00  \n",
      "25%          5.638140e+06  \n",
      "50%          1.194786e+07  \n",
      "75%          2.080371e+07  \n",
      "max          2.661120e+07  \n",
      "\n",
      "[8 rows x 236 columns]\n",
      "CPU times: user 1min 35s, sys: 3.54 s, total: 1min 38s\n",
      "Wall time: 7.12 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print(time.asctime())\n",
    "\n",
    "## Lst us print the description of training data\n",
    "print(original_train_df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting target feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target feature (\"isAnomaly\") need to be seperated from train data. So this is extracted to a seperate dataframe \n",
    "\"anomalies_df\". After separation,the train data is saved to pickle \"trainpickle\" and anomaly data(train target) is \n",
    "saved to pickle \"anomaliespickle\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Nov 21 15:38:40 2018\n",
      "\n",
      "training data shape excluding anomalies (750199, 236)\n",
      "\n",
      "anomalies data shape (750199,)\n",
      "counts of label 'True' in anomalies: 14363\n",
      "counts of label 'False' in anomalies: 735836\n",
      "CPU times: user 785 ms, sys: 2.91 s, total: 3.69 s\n",
      "Wall time: 7.39 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print(time.asctime())\n",
    "\n",
    "input_variables = [variable for variable in list(original_train_df.columns.values) if variable != output_variable]\n",
    "anomalies_df = original_train_df[output_variable]\n",
    "train_df = original_train_df[input_variables]\n",
    "print(\"\\ntraining data shape excluding anomalies\", train_df.shape)\n",
    "print(\"\\nanomalies data shape\", anomalies_df.shape)\n",
    "print(\"counts of label 'True' in anomalies: {}\".format(sum(anomalies_df == True)))\n",
    "print(\"counts of label 'False' in anomalies: {}\".format(sum(anomalies_df == False)))\n",
    "\n",
    "train_df.to_pickle(\"trainpickle\")\n",
    "anomalies_df.to_pickle(\"anomaliespickle\")\n",
    "\n",
    "del original_train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"isAnomaly\" feature is having boolean \"True\" and \"False\". Let us replace it with 1 and 0 respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Nov 21 15:38:48 2018\n",
      "counts of label 'True' in anomalies: 14363\n",
      "counts of label 'False' in anomalies: 735836\n",
      "CPU times: user 2.19 s, sys: 111 ms, total: 2.3 s\n",
      "Wall time: 297 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print(time.asctime())\n",
    "\n",
    "## Replacing True with 1 and false with 0 in anomalies data frame\n",
    "anomalies_df = anomalies_df.replace(True, 1)\n",
    "anomalies_df = anomalies_df.replace(False, 0)\n",
    "print(\"counts of label 'True' in anomalies: {}\".format(sum(anomalies_df == 1)))\n",
    "print(\"counts of label 'False' in anomalies: {}\".format(sum(anomalies_df == 0)))\n",
    "\n",
    "anomalies_df.to_pickle(\"anomaliespickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering out non significant features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature filtering: Following feature filtering rules are applied.\n",
    "1) All null features are removed(this rule will anyways be fulfilled in the next rule(2)).\n",
    "2) All features with null values greater than null_val_threshold(90%)are removed.\n",
    "3) All features with single unique values are removed.\n",
    "After applying the above rules, 67 features are selected for dropping. So they are dropped and remaining number of features have become 169.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Nov 21 15:38:48 2018\n",
      "\n",
      "length of columns to drop 67\n",
      "\n",
      "shape of train df after dropping few columns (750199, 169)\n",
      "\n",
      "memory footprint of train df = 0.945GB\n",
      "CPU times: user 34.2 s, sys: 3.35 s, total: 37.5 s\n",
      "Wall time: 9.94 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print(time.asctime())\n",
    "\n",
    "## columns to drop if all feature values are null\n",
    "## columns to drop if there are more than 90% null values in the feature\n",
    "## columns to drop if all values are unique in the feature\n",
    "cols_to_drop = [col for col in train_df.columns \n",
    "                if ((train_df[col].isnull().all())                                                 \n",
    "                    or (((train_df[col].isnull().sum(axis = 0)/len(train_df[col])) * 100) >  null_val_threshold)\n",
    "                    or (len(train_df[col].unique()) == unique_threshold))]\n",
    "print(\"\\nlength of columns to drop\", len(cols_to_drop))\n",
    "\n",
    "## Dropping the identified features till now\n",
    "train_df.drop(columns=cols_to_drop, inplace=True)\n",
    "print(\"\\nshape of train df after dropping few columns\", train_df.shape)\n",
    "print (\"\\nmemory footprint of train df = %0.3fGB\" %(train_df.memory_usage(deep=True).sum()/1024**3))\n",
    "\n",
    "train_df.to_pickle(\"trainpickle\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are there null values still. If so, either those samples are to removed or data has to be imputed with some values (either 0 or most frequent values of the feature). But, no null values are observed. So no data imputation techniques need to be followed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Nov 21 15:38:58 2018\n",
      "False\n",
      "CPU times: user 1.91 s, sys: 81.3 ms, total: 1.99 s\n",
      "Wall time: 382 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print(time.asctime())\n",
    "\n",
    "## Is there any NULL values in training data?\n",
    "## If there are NULL values, then some imputation methods are to be employed.\n",
    "print(train_df.isnull().values.any())\n",
    "\n",
    "## Fill missing values with 0\n",
    "train_df.fillna(0, inplace=True)\n",
    "##print(\"\\nshape of train df after dropping few columns\", train_df.shape)\n",
    "##print (\"\\nmemory footprint of train df = %0.3fGB\" %(train_df.memory_usage(deep=True).sum()/1024**3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training data description indicated a need for variance filtering on training data. Used VarianceThreshold API from sklearn.feature_selection for such a filtering. This reduced the number of features to 63 features. So training will be done with these 63 features. Note that the we have identified that only 63 out of original 236 features may have impact on target predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Nov 21 15:38:58 2018\n",
      "\n",
      "shape of train df after applying variance (750199, 63)\n",
      "\n",
      "Memory footprint of train df = 0.352GB\n",
      "CPU times: user 7.45 s, sys: 2.13 s, total: 9.58 s\n",
      "Wall time: 2.22 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print(time.asctime())\n",
    "\n",
    "## identifying and dropping features with vairance below variance_threshold\n",
    "Var = VarianceThreshold(threshold=variance_threshold).fit(train_df)\n",
    "indx  = Var.get_support(indices=True) \n",
    "train_df = train_df.iloc[:,indx]\n",
    "print(\"\\nshape of train df after applying variance\", train_df.shape)\n",
    "print (\"\\nMemory footprint of train df = %0.3fGB\" %(train_df.memory_usage(deep=True).sum()/1024**3))\n",
    "\n",
    "train_df.to_pickle(\"trainpickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling of feature values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature scaling: The data description shows the need for feature scaling. Originally we didn't use any feature scaling technique. We have done the training of those 63 feature samples using KNN(K-Nearest Neighbor) classifier after splitting the data into fitting and validation set. Below is the the result that we have obtained.\n",
    "\n",
    "F1 (weighted) = 0.9796880998298155\n",
    "\n",
    "Precision (None) = [0.98587054 0.640599  ]\n",
    "\n",
    "Recall (None) = [0.99706456 0.26801253]\n",
    "\n",
    "F1 (None) = [0.99143596 0.37791411]\n",
    "\n",
    "confusion_matrix = [[146735    432]\n",
    "                    [  2103    770]]\n",
    "                    \n",
    "                    \n",
    "But then the feature scaling started to give better predictions. We used MinMaxScaler from sklearn.preprocessing for feature scaling. The value are scaled in the range o to 1. The results can be viewed from later cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Nov 21 15:39:01 2018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.7/site-packages/sklearn/preprocessing/data.py:323: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "shape of full chunk df after minmax scaling (750199, 63)\n",
      "\n",
      "Memory footprint of minmax scaled chunk df = 0.352GB\n",
      "\n",
      "scaled train data description           Unnamed: 0   Unnamed: 0.1           host        process  \\\n",
      "count  750199.000000  750199.000000  750199.000000  750199.000000   \n",
      "mean        0.500000       0.352888       0.481061       0.500103   \n",
      "std         0.288676       0.210618       0.316347       0.500000   \n",
      "min         0.000000       0.000000       0.000000       0.000000   \n",
      "25%         0.250000       0.173422       0.222222       0.000000   \n",
      "50%         0.500000       0.347029       0.444444       1.000000   \n",
      "75%         0.750000       0.520822       0.777778       1.000000   \n",
      "max         1.000000       1.000000       1.000000       1.000000   \n",
      "\n",
      "       Heap usage activity : (d/dx (MXBean(java.lang:type=Memory).HeapMemoryUsage.used))  \\\n",
      "count                                      750199.000000                                   \n",
      "mean                                            0.662371                                   \n",
      "std                                             0.027317                                   \n",
      "min                                             0.000000                                   \n",
      "25%                                             0.662312                                   \n",
      "50%                                             0.665683                                   \n",
      "75%                                             0.666552                                   \n",
      "max                                             1.000000                                   \n",
      "\n",
      "       Connection delay : (MXBean(com.bea:Name=source10,Type=JDBCDataSourceRuntime).ConnectionDelayTime)  \\\n",
      "count                                      750199.000000                                                   \n",
      "mean                                            0.051821                                                   \n",
      "std                                             0.125297                                                   \n",
      "min                                             0.000000                                                   \n",
      "25%                                             0.000000                                                   \n",
      "50%                                             0.000000                                                   \n",
      "75%                                             0.000000                                                   \n",
      "max                                             1.000000                                                   \n",
      "\n",
      "       Stuck threads : (MXBean(com.bea:Name=ThreadPoolRuntime,Type=ThreadPoolRuntime).StuckThreadCount)  \\\n",
      "count                                      750199.000000                                                  \n",
      "mean                                            0.003482                                                  \n",
      "std                                             0.019340                                                  \n",
      "min                                             0.000000                                                  \n",
      "25%                                             0.000000                                                  \n",
      "50%                                             0.000000                                                  \n",
      "75%                                             0.000000                                                  \n",
      "max                                             1.000000                                                  \n",
      "\n",
      "       DB connection started : (incld/dx (MXBean(com.bea:Name=source09,Type=JDBCConnectionPoolRuntime).ConnectionsTotalCount))  \\\n",
      "count                                      750199.000000                                                                         \n",
      "mean                                            0.001931                                                                         \n",
      "std                                             0.017354                                                                         \n",
      "min                                             0.000000                                                                         \n",
      "25%                                             0.000000                                                                         \n",
      "50%                                             0.000000                                                                         \n",
      "75%                                             0.000000                                                                         \n",
      "max                                             1.000000                                                                         \n",
      "\n",
      "       Connection delay : (MXBean(com.bea:Name=source06,Type=JDBCDataSourceRuntime).ConnectionDelayTime)  \\\n",
      "count                                      750199.000000                                                   \n",
      "mean                                            0.013929                                                   \n",
      "std                                             0.035450                                                   \n",
      "min                                             0.000000                                                   \n",
      "25%                                             0.011653                                                   \n",
      "50%                                             0.012385                                                   \n",
      "75%                                             0.013117                                                   \n",
      "max                                             1.000000                                                   \n",
      "\n",
      "       Prepared statement cache hit rate : ((MXBean(com.bea:Name=source01,Type=JDBCDataSourceRuntime).PrepStmtCacheHitCount / MXBean(com.bea:Name=source01,Type=JDBCDataSourceRuntime).PrepStmtCacheMissCount))  \\\n",
      "count                                      750199.000000                                                                                                                                                          \n",
      "mean                                            0.121215                                                                                                                                                          \n",
      "std                                             0.133545                                                                                                                                                          \n",
      "min                                             0.000000                                                                                                                                                          \n",
      "25%                                             0.078351                                                                                                                                                          \n",
      "50%                                             0.078351                                                                                                                                                          \n",
      "75%                                             0.078351                                                                                                                                                          \n",
      "max                                             1.000000                                                                                                                                                          \n",
      "\n",
      "              ...          \\\n",
      "count         ...           \n",
      "mean          ...           \n",
      "std           ...           \n",
      "min           ...           \n",
      "25%           ...           \n",
      "50%           ...           \n",
      "75%           ...           \n",
      "max           ...           \n",
      "\n",
      "       Rel. unavailable connections : ((MXBean(com.bea:Name=source09,Type=JDBCConnectionPoolRuntime).NumUnavailable / MXBean(com.bea:Name=source09,Type=JDBCConnectionPoolRuntime).CurrCapacity))  \\\n",
      "count                                      750199.000000                                                                                                                                            \n",
      "mean                                            0.108094                                                                                                                                            \n",
      "std                                             0.207412                                                                                                                                            \n",
      "min                                             0.000000                                                                                                                                            \n",
      "25%                                             0.000000                                                                                                                                            \n",
      "50%                                             0.000000                                                                                                                                            \n",
      "75%                                             0.069444                                                                                                                                            \n",
      "max                                             1.000000                                                                                                                                            \n",
      "\n",
      "       Connection delay : (MXBean(com.bea:Name=source03,Type=JDBCConnectionPoolRuntime).ConnectionDelayTime)  \\\n",
      "count                                      750199.000000                                                       \n",
      "mean                                            0.050873                                                       \n",
      "std                                             0.059288                                                       \n",
      "min                                             0.000000                                                       \n",
      "25%                                             0.040555                                                       \n",
      "50%                                             0.045538                                                       \n",
      "75%                                             0.049097                                                       \n",
      "max                                             1.000000                                                       \n",
      "\n",
      "       Rel. unavailable connections : ((MXBean(com.bea:Name=source09,Type=JDBCDataSourceRuntime).NumUnavailable / MXBean(com.bea:Name=source09,Type=JDBCDataSourceRuntime).CurrCapacity))  \\\n",
      "count                                      750199.000000                                                                                                                                    \n",
      "mean                                            0.096073                                                                                                                                    \n",
      "std                                             0.184371                                                                                                                                    \n",
      "min                                             0.000000                                                                                                                                    \n",
      "25%                                             0.000000                                                                                                                                    \n",
      "50%                                             0.000000                                                                                                                                    \n",
      "75%                                             0.061728                                                                                                                                    \n",
      "max                                             1.000000                                                                                                                                    \n",
      "\n",
      "       Connection delay : (MXBean(com.bea:Name=source06,Type=JDBCConnectionPoolRuntime).ConnectionDelayTime)  \\\n",
      "count                                      750199.000000                                                       \n",
      "mean                                            0.013929                                                       \n",
      "std                                             0.035450                                                       \n",
      "min                                             0.000000                                                       \n",
      "25%                                             0.011653                                                       \n",
      "50%                                             0.012385                                                       \n",
      "75%                                             0.013117                                                       \n",
      "max                                             1.000000                                                       \n",
      "\n",
      "       Last GC duration : (MXBean(java.lang:name=PS MarkSweep,type=GarbageCollector).LastGcInfo.duration)  \\\n",
      "count                                      750199.000000                                                    \n",
      "mean                                            0.021157                                                    \n",
      "std                                             0.027705                                                    \n",
      "min                                             0.000000                                                    \n",
      "25%                                             0.009148                                                    \n",
      "50%                                             0.013503                                                    \n",
      "75%                                             0.019625                                                    \n",
      "max                                             1.000000                                                    \n",
      "\n",
      "       DB connection started : (incld/dx (MXBean(com.bea:Name=source09,Type=JDBCDataSourceRuntime).ConnectionsTotalCount))  \\\n",
      "count                                      750199.000000                                                                     \n",
      "mean                                            0.001971                                                                     \n",
      "std                                             0.017691                                                                     \n",
      "min                                             0.000000                                                                     \n",
      "25%                                             0.000000                                                                     \n",
      "50%                                             0.000000                                                                     \n",
      "75%                                             0.000000                                                                     \n",
      "max                                             1.000000                                                                     \n",
      "\n",
      "       Connection delay : (MXBean(com.bea:Name=source08,Type=JDBCDataSourceRuntime).ConnectionDelayTime)  \\\n",
      "count                                      750199.000000                                                   \n",
      "mean                                            0.003877                                                   \n",
      "std                                             0.018183                                                   \n",
      "min                                             0.000000                                                   \n",
      "25%                                             0.000000                                                   \n",
      "50%                                             0.000000                                                   \n",
      "75%                                             0.000000                                                   \n",
      "max                                             1.000000                                                   \n",
      "\n",
      "       Process CPU : (\\Process(java)\\CPU)  \\\n",
      "count                       750199.000000   \n",
      "mean                             0.013836   \n",
      "std                              0.047945   \n",
      "min                              0.000000   \n",
      "25%                              0.000000   \n",
      "50%                              0.000000   \n",
      "75%                              0.001285   \n",
      "max                              1.000000   \n",
      "\n",
      "       Connection delay : (MXBean(com.bea:Name=source02,Type=JDBCDataSourceRuntime).ConnectionDelayTime)  \\\n",
      "count                                      750199.000000                                                   \n",
      "mean                                            0.021974                                                   \n",
      "std                                             0.056863                                                   \n",
      "min                                             0.000000                                                   \n",
      "25%                                             0.015344                                                   \n",
      "50%                                             0.017755                                                   \n",
      "75%                                             0.018826                                                   \n",
      "max                                             1.000000                                                   \n",
      "\n",
      "       seconds_from_start  \n",
      "count       750199.000000  \n",
      "mean             0.487962  \n",
      "std              0.307883  \n",
      "min              0.000000  \n",
      "25%              0.211871  \n",
      "50%              0.448979  \n",
      "75%              0.781765  \n",
      "max              1.000000  \n",
      "\n",
      "[8 rows x 63 columns]\n",
      "CPU times: user 32.4 s, sys: 2.4 s, total: 34.8 s\n",
      "Wall time: 3.87 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print(time.asctime())\n",
    "\n",
    "## applying scaling on train data set, range (0,1)\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "scaled_train_df = scaler.fit_transform(train_df)\n",
    "scaled_train_df = pd.DataFrame(scaled_train_df, columns=train_df.columns)\n",
    "print(\"\\nshape of full chunk df after minmax scaling\", scaled_train_df.shape)\n",
    "print (\"\\nMemory footprint of minmax scaled chunk df = %0.3fGB\" %(scaled_train_df.memory_usage(deep=True).sum()/1024**3))   \n",
    "## Write the resultant dataframe to a pickle\n",
    "scaled_train_df.to_pickle(\"scaledtrainpickle\")\n",
    "\n",
    "print(\"\\nscaled train data description\", scaled_train_df.describe())\n",
    "\n",
    "del train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See below the description of scaled data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, its time to split the train data to fitting and validation data. We used train_test_split from sklearn.model_selection for this purpose. The test_size is set as 20%. Here is the fitting and validation data shapes after splitting.\n",
    "\n",
    "training input shape is (600159, 63)\n",
    "\n",
    "validation input shape is (150040, 63)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Nov 21 15:39:05 2018\n",
      "training input is \n",
      " (600159, 63)\n",
      "training output is \n",
      " (600159,)\n",
      "validation input is \n",
      " (150040, 63)\n",
      "validation output is \n",
      " (150040,)\n",
      "counts of label 'True' in training set: 11490\n",
      "counts of label 'False' in training set: 588669\n",
      "counts of label 'True' in validation set: 2873\n",
      "counts of label 'False' in validation set: 147167\n",
      "CPU times: user 3.89 s, sys: 879 ms, total: 4.77 s\n",
      "Wall time: 2.28 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print(time.asctime())\n",
    "\n",
    "train_df  = pd.read_pickle(\"scaledtrainpickle\")\n",
    "## Splitting into training and validation set\n",
    "sets = train_test_split(train_df, anomalies_df, test_size=0.20, stratify=anomalies_df)\n",
    "X_train = sets[0]\n",
    "y_train = sets[2]\n",
    "X_test  = sets[1]\n",
    "y_test  = sets[3]\n",
    "\n",
    "## shape of training and validation set\n",
    "print(\"training input is \\n\", X_train.shape)\n",
    "print(\"training output is \\n\",y_train.shape)\n",
    "print(\"validation input is \\n\", X_test.shape)\n",
    "print(\"validation output is \\n\", y_test.shape)\n",
    "\n",
    "## No of \"anomalies\" and \"no anomalies\" in training set and validation set\n",
    "print(\"counts of label 'True' in training set: {}\".format(sum(y_train == 1)))\n",
    "print(\"counts of label 'False' in training set: {}\".format(sum(y_train == 0)))\n",
    "print(\"counts of label 'True' in validation set: {}\".format(sum(y_test == 1)))\n",
    "print(\"counts of label 'False' in validation set: {}\".format(sum(y_test == 0)))\n",
    "\n",
    "X_train.to_pickle(\"traininginputpickle\")\n",
    "y_train.to_pickle(\"trainingoutputpickle\")\n",
    "X_test.to_pickle(\"validationinputpickle\")\n",
    "y_test.to_pickle(\"validationoutputpickle\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling techniques employed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is observed that there is a heavy imbalance between majority to minority samples in training data (as can be seen in the previous cell output). The ratio of minority to majority is 1.95%. So attempting different sampling techniques so as increase the share of minority samples in training. Here, we made use of oversampling and undersampling techniques provided by imblearn(imbalanced learning).\n",
    "\n",
    "Below you can see the following oversampling methods used.\n",
    "1) Random oversampling\n",
    "2) SMOTE oversampling\n",
    "\n",
    "Below you can see the following undersampling methods used.\n",
    "1) Random undersampling\n",
    "2) Tomeklink undersampling over the SMOTE oversampled data\n",
    "\n",
    "The results of different trials are in the cells following.The results without sampling, with oversampling, with undersampling etc can be analysed from later cells. Note that sampling is applied over the scaled data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the below cell, random oversampling is done.\n",
    "After random oversampling, the number of minority samples in fitting set are increased from 11490 to 194260. The ratio of minority to majority samples is henceforth made 1:3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Nov 21 15:39:07 2018\n",
      "Memory footprint of train data after ROS= 0.367GB\n",
      "\n",
      "shape of training data after ROS (782929, 63)\n",
      "shape of target data after ROS (782929,)\n",
      "counts of label 'True' in training target: 194260\n",
      "counts of label 'False' in training target: 588669\n",
      "CPU times: user 10.3 s, sys: 928 ms, total: 11.2 s\n",
      "Wall time: 5.35 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print(time.asctime())\n",
    "\n",
    "## Apply random oversampling\n",
    "## Sampling ration of minority to majority class is 1:3\n",
    "ros = RandomOverSampler(sampling_strategy='minority', return_indices=True, ratio=0.33)\n",
    "X_ros, y_ros, id_ros = ros.fit_sample(X_train, y_train)\n",
    "X_ros = pd.DataFrame(X_ros)\n",
    "X_ros.columns = X_train.columns\n",
    "print(\"Memory footprint of train data after ROS= %0.3fGB\\n\" %(X_ros.memory_usage(deep=True).sum()/1024**3))\n",
    "print(\"shape of training data after ROS\", X_ros.shape)\n",
    "print(\"shape of target data after ROS\", y_ros.shape)\n",
    "print(\"counts of label 'True' in training target: {}\".format(sum(y_ros == 1)))\n",
    "print(\"counts of label 'False' in training target: {}\".format(sum(y_ros == 0)))\n",
    "y_ros = pd.DataFrame(y_ros)\n",
    "X_ros.to_pickle(\"rostraininginputpickle\")\n",
    "y_ros.to_pickle(\"rostrainingoutputpickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the below cell, SMOTE oversampling is done. After SMOTE oversampling, the number of minority samples in fitting set are increased from 11490 to 194260. The ratio of minority to majority samples is henceforth made 1:3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Nov 21 15:39:12 2018\n",
      "shape of training data after SMOTE (782929, 63)\n",
      "Memory footprint of train data after SMOTE= 0.367GB\n",
      "shape of target data after SMOTE (782929,)\n",
      "counts of label 'True' in testing target: 194260\n",
      "counts of label 'False' in testing target: 588669\n",
      "CPU times: user 24.9 s, sys: 1.43 s, total: 26.3 s\n",
      "Wall time: 7.44 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print(time.asctime())\n",
    "\n",
    "## Apply SMOTE oversampling\n",
    "smt = SMOTE(sampling_strategy='minority', n_jobs = -1, ratio = 0.33)\n",
    "X_smote, y_smote = smt.fit_sample(X_train, y_train)\n",
    "X_smote = pd.DataFrame(X_smote)\n",
    "print(\"shape of training data after SMOTE\", X_smote.shape)\n",
    "print(\"Memory footprint of train data after SMOTE= %0.3fGB\" %(X_smote.memory_usage(deep=True).sum()/1024**3))\n",
    "print(\"shape of target data after SMOTE\", y_smote.shape)\n",
    "print(\"counts of label 'True' in testing target: {}\".format(sum(y_smote == 1)))\n",
    "print(\"counts of label 'False' in testing target: {}\".format(sum(y_smote == 0)))\n",
    "\n",
    "X_smote.columns = X_train.columns\n",
    "X_smote.to_pickle(\"smotetraininginputpickle\")\n",
    "\n",
    "y_smote = pd.DataFrame(y_smote)\n",
    "y_smote.to_pickle(\"smotetrainingoutputpickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the below cell, random undersampling of majority samples are attempted. The majority samples count is decreased from 588669 to 57450. The ratio of minority to majority samples are henceforth made 1:5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Nov 21 15:39:20 2018\n",
      "Memory footprint of train data after RUS= 0.032GB\n",
      "\n",
      "shape of training data after RUS (68940, 63)\n",
      "shape of target data after RUS (68940,)\n",
      "counts of label 'True' in training target: 11490\n",
      "counts of label 'False' in training target: 57450\n",
      "CPU times: user 4 s, sys: 175 ms, total: 4.17 s\n",
      "Wall time: 814 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print(time.asctime())\n",
    "\n",
    "## Apply random undersampling\n",
    "## Sampling ration of minority to majority class is 1:5\n",
    "rus = RandomUnderSampler(return_indices=True, ratio=0.20)\n",
    "X_rus, y_rus, id_rus = rus.fit_sample(X_train, y_train)\n",
    "X_rus = pd.DataFrame(X_rus)\n",
    "X_rus.columns = X_train.columns\n",
    "print(\"Memory footprint of train data after RUS= %0.3fGB\\n\" %(X_rus.memory_usage(deep=True).sum()/1024**3))\n",
    "print(\"shape of training data after RUS\", X_rus.shape)\n",
    "print(\"shape of target data after RUS\", y_rus.shape)\n",
    "print(\"counts of label 'True' in training target: {}\".format(sum(y_rus == 1)))\n",
    "print(\"counts of label 'False' in training target: {}\".format(sum(y_rus == 0)))\n",
    "y_rus = pd.DataFrame(y_rus)\n",
    "X_rus.to_pickle(\"rustraininginputpickle\")\n",
    "y_rus.to_pickle(\"rustrainingoutputpickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the below cell, Tomeklink undersampling of SMOTE data is attempted. The sampling strategy used is \"all\". The ratio of minority to majority samples is maintained near to 1:3, removing the tomek links from SMOTE oversampled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Nov 21 15:39:20 2018\n",
      "shape of training data after TL (782927, 63)\n",
      "Memory footprint of train data after TL= 0.367GB\n",
      "\n",
      "shape of target data after TL (782927,)\n",
      "counts of label 'True' in training target: 194259\n",
      "counts of label 'False' in training target: 588668\n",
      "CPU times: user 45min 3s, sys: 20.1 s, total: 45min 23s\n",
      "Wall time: 9min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print(time.asctime())\n",
    "\n",
    "## Apply Tomeklink undersampling over Smote\n",
    "y = y_smote.iloc[:,0]\n",
    "tl = TomekLinks(return_indices=True,n_jobs = -1, sampling_strategy = 'all')\n",
    "X_tlink, y_tlink, idx_resampled = tl.fit_sample(X_smote, y)\n",
    "print(\"shape of training data after TL\", X_tlink.shape)\n",
    "\n",
    "X_tlink = pd.DataFrame(X_tlink)\n",
    "X_tlink.columns = X_smote.columns\n",
    "print(\"Memory footprint of train data after TL= %0.3fGB\\n\" %(X_tlink.memory_usage(deep=True).sum()/1024**3))\n",
    "print(\"shape of target data after TL\", y_tlink.shape)\n",
    "print(\"counts of label 'True' in training target: {}\".format(sum(y_tlink == 1)))\n",
    "print(\"counts of label 'False' in training target: {}\".format(sum(y_tlink == 0)))\n",
    "\n",
    "y_tlink = pd.DataFrame(y_tlink)\n",
    "X_tlink.to_pickle(\"tlinktraininginputpickle\")\n",
    "y_tlink.to_pickle(\"tlinktrainingoutputpickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbor classifier for training and validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here(in below cell), we have build a KNN model using the unsampled data (ofcourse this is the scaled data using MinMaxScaler). We have checked the predictions with \"n_neighbors\" parameter in the range 1 to 10. The \"weights\" parameter is set to \"distance\". Used n_jobs = -1 so as to make use of all the available cores for execution. The best result observed was with n_neighbors = 1 & 2. \n",
    "\n",
    "The result was like this.\n",
    "#neighbors = 1, f1_score for both classes = [0.99962628 0.98083624]\n",
    "\n",
    "#neighbors = 2, f1_score for both classes = [0.99962628 0.98083624]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Nov 21 15:48:42 2018\n",
      "#neighbors = 1, f1_score for both classes = [0.99962628 0.98083624]\n",
      "#neighbors = 2, f1_score for both classes = [0.99962628 0.98083624]\n",
      "#neighbors = 3, f1_score for both classes = [0.99960592 0.97972737]\n",
      "#neighbors = 4, f1_score for both classes = [0.99961611 0.98025511]\n",
      "#neighbors = 5, f1_score for both classes = [0.99957876 0.97829132]\n",
      "#neighbors = 6, f1_score for both classes = [0.99958896 0.97879797]\n",
      "#neighbors = 7, f1_score for both classes = [0.99956858 0.97773102]\n",
      "#neighbors = 8, f1_score for both classes = [0.99954819 0.97669529]\n",
      "#neighbors = 9, f1_score for both classes = [0.99954481 0.97648298]\n",
      "#neighbors = 10, f1_score for both classes = [0.99953123 0.97576396]\n",
      "the best f1 score is [0.99962628 0.98083624]\n",
      "CPU times: user 2h 9min 6s, sys: 1min 24s, total: 2h 10min 31s\n",
      "Wall time: 1h 6min 55s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%%time\n",
    "\n",
    "print(time.asctime())\n",
    "\n",
    "## Fit train data to KNN estimator\n",
    "best_f1_knn = [0,0]\n",
    "for N in range(1,11):\n",
    "    knn = neighbors.KNeighborsClassifier(n_neighbors=N, weights='distance', n_jobs=-1)\n",
    "    knn.fit(X_train, y_train) \n",
    "    predictions = knn.predict(X_test)\n",
    "    f1_knn = metrics.f1_score(y_true=y_test, y_pred=predictions, average=None)\n",
    "    knn_params = knn.get_params(deep=True)\n",
    "    print(\"#neighbors = {}, f1_score for both classes = {}\".format(N, f1_knn))\n",
    "    if (f1_knn[1] > best_f1_knn[1]):\n",
    "        best_f1_knn = f1_knn\n",
    "print(\"the best f1 score is\", best_f1_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here(in below cell), we have build a KNN model using the random undersampled data. We have checked the predictions with \"n_neighbors\" parameter in the range 1 to 10. The \"weights\" parameter is set to \"distance\". Used n_jobs = -1 so as to make use of all the available cores for execution. The best result observed was with n_neighbors = 1 & 2.\n",
    "\n",
    "The result was like this.\n",
    "\n",
    "#neighbors = 1, f1_score for both classes = [0.99858139 0.93198499]\n",
    "\n",
    "#neighbors = 2, f1_score for both classes = [0.99858139 0.93198499]\n",
    "\n",
    "It can be noted that these results are weaker (for both the majority and minority classes) when comparing to predictions made over unsampled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Nov 21 16:55:37 2018\n",
      "#neighbors = 1, f1_score for both classes = [0.99858139 0.93198499]\n",
      "#neighbors = 2, f1_score for both classes = [0.99858139 0.93198499]\n",
      "#neighbors = 3, f1_score for both classes = [0.99827134 0.91819646]\n",
      "#neighbors = 4, f1_score for both classes = [0.998377   0.92280304]\n",
      "#neighbors = 5, f1_score for both classes = [0.99814869 0.91273661]\n",
      "#neighbors = 6, f1_score for both classes = [0.99820325 0.91505792]\n",
      "#neighbors = 7, f1_score for both classes = [0.99800549 0.90659866]\n",
      "#neighbors = 8, f1_score for both classes = [0.99804985 0.90842257]\n",
      "#neighbors = 9, f1_score for both classes = [0.99785544 0.90022173]\n",
      "#neighbors = 10, f1_score for both classes = [0.99781794 0.89859199]\n",
      "the best f1 score is [0.99858139 0.93198499]\n",
      "CPU times: user 22min 29s, sys: 1min 8s, total: 23min 37s\n",
      "Wall time: 1min 54s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%%time\n",
    "\n",
    "print(time.asctime())\n",
    "\n",
    "## Fit RUS train data to KNN estimator\n",
    "best_f1_knn_rus = [0,0]\n",
    "for N in range(1,11):\n",
    "    knn = neighbors.KNeighborsClassifier(n_neighbors=N, weights='distance', n_jobs=-1)\n",
    "    knn.fit(X_rus, y_rus.iloc[:,0]) \n",
    "    predictions = knn.predict(X_test)\n",
    "    f1_knn_rus = metrics.f1_score(y_true=y_test, y_pred=predictions, average=None)\n",
    "    knn_params = knn.get_params(deep=True)\n",
    "    print(\"#neighbors = {}, f1_score for both classes = {}\".format(N, f1_knn_rus))\n",
    "    if (f1_knn_rus[1] > best_f1_knn_rus[1]):\n",
    "        best_f1_knn_rus = f1_knn_rus\n",
    "print(\"the best f1 score is\", best_f1_knn_rus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here(in below cell), we have build a KNN model using the random oversampled data. We have checked the predictions with \"n_neighbors\" parameter in the range 1 to 10. The \"weights\" parameter is set to \"distance\". Used n_jobs = -1 so as to make use of all the available cores for execution. The best result observed was with n_neighbors = 1 & 2.\n",
    "\n",
    "The result was like this.\n",
    "\n",
    "#neighbors = 1, f1_score for both classes = [0.99962628 0.98083624]\n",
    "\n",
    "#neighbors = 2, f1_score for both classes = [0.99962628 0.98083624]\n",
    "\n",
    "It can be noted that these results are in par (for both the majority and minority classes) with the predictions made over unsampled data. This shows that oversamping of minority samples has no any effect on the outcome of predictions. This is found a bit surprising, and we are not able to provide any good reasoning for the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Nov 21 16:57:32 2018\n",
      "#neighbors = 1, f1_score for both classes = [0.99962628 0.98083624]\n",
      "#neighbors = 2, f1_score for both classes = [0.99962628 0.98083624]\n",
      "#neighbors = 3, f1_score for both classes = [0.99952084 0.9757607 ]\n",
      "#neighbors = 4, f1_score for both classes = [0.99951404 0.97542533]\n",
      "#neighbors = 5, f1_score for both classes = [0.99941879 0.97084399]\n",
      "#neighbors = 6, f1_score for both classes = [0.99940519 0.97018231]\n",
      "#neighbors = 7, f1_score for both classes = [0.99932353 0.96629975]\n",
      "#neighbors = 8, f1_score for both classes = [0.99929291 0.96484111]\n",
      "#neighbors = 9, f1_score for both classes = [0.99919424 0.96014797]\n",
      "#neighbors = 10, f1_score for both classes = [0.9991534  0.95822849]\n",
      "the best f1 score is [0.99962628 0.98083624]\n",
      "CPU times: user 2h 37min 9s, sys: 1min 25s, total: 2h 38min 35s\n",
      "Wall time: 1h 28min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print(time.asctime())\n",
    "\n",
    "## Fit ROS train data to KNN estimator\n",
    "best_f1_knn_ros = [0,0]\n",
    "for N in range(1,11):\n",
    "    knn = neighbors.KNeighborsClassifier(n_neighbors=N, weights='distance', n_jobs=-1)\n",
    "    knn.fit(X_ros, y_ros.iloc[:,0]) \n",
    "    predictions = knn.predict(X_test)\n",
    "    f1_knn_ros = metrics.f1_score(y_true=y_test, y_pred=predictions, average=None)\n",
    "    knn_params = knn.get_params(deep=True)\n",
    "    print(\"#neighbors = {}, f1_score for both classes = {}\".format(N, f1_knn_ros))\n",
    "    if (f1_knn_ros[1] > best_f1_knn_ros[1]):\n",
    "        best_f1_knn_ros = f1_knn_ros\n",
    "print(\"the best f1 score is\", best_f1_knn_ros)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here(in below cell), we have build a KNN model using the SMOTE oversampled data. We have checked the predictions with \"n_neighbors\" parameter in the range 1 to 10. The \"weights\" parameter is set to \"distance\". Used n_jobs = -1 so as to make use of all the available cores for execution. The best result observed was with n_neighbors = 1 & 2.\n",
    "\n",
    "The result was like this.\n",
    "\n",
    "#neighbors = 1, f1_score for both classes = [0.99960245 0.97974727]\n",
    "\n",
    "#neighbors = 2, f1_score for both classes = [0.99960245 0.97974727]\n",
    "\n",
    "It can be noted that these results are in par with (for both the majority and minority classes) with the predictions made over unsampled data (as well as the random oversampled data). This as well is found a bit surprising, and we are not able to provide any good reasoning for why oversampling of minority classes are not improving the predictions on this data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Nov 21 18:26:06 2018\n",
      "#neighbors = 1, f1_score for both classes = [0.99960245 0.97974727]\n",
      "#neighbors = 2, f1_score for both classes = [0.99960245 0.97974727]\n",
      "#neighbors = 3, f1_score for both classes = [0.9994732  0.97349974]\n",
      "#neighbors = 4, f1_score for both classes = [0.99949701 0.97466621]\n",
      "#neighbors = 5, f1_score for both classes = [0.99934054 0.96711864]\n",
      "#neighbors = 6, f1_score for both classes = [0.99935075 0.96758866]\n",
      "#neighbors = 7, f1_score for both classes = [0.99925548 0.96307537]\n",
      "#neighbors = 8, f1_score for both classes = [0.99925208 0.96290051]\n",
      "#neighbors = 9, f1_score for both classes = [0.9991432 0.9577323]\n",
      "#neighbors = 10, f1_score for both classes = [0.99913299 0.95726496]\n",
      "the best f1 score is [0.99960245 0.97974727]\n",
      "CPU times: user 2h 14min 30s, sys: 1min 22s, total: 2h 15min 53s\n",
      "Wall time: 1h 11min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print(time.asctime())\n",
    "\n",
    "## Fit SMOTE train data to KNN estimator\n",
    "best_f1_knn_smote = [0,0]\n",
    "for N in range(1,11):\n",
    "    knn = neighbors.KNeighborsClassifier(n_neighbors=N, weights='distance', n_jobs=-1)\n",
    "    knn.fit(X_smote, y_smote.iloc[:,0]) \n",
    "    predictions = knn.predict(X_test)\n",
    "    f1_knn_smote = metrics.f1_score(y_true=y_test, y_pred=predictions, average=None)\n",
    "    knn_params = knn.get_params(deep=True)\n",
    "    print(\"#neighbors = {}, f1_score for both classes = {}\".format(N, f1_knn_smote))\n",
    "    if (f1_knn_smote[1] > best_f1_knn_smote[1]):\n",
    "        best_f1_knn_smote = f1_knn_smote\n",
    "print(\"the best f1 score is\", best_f1_knn_smote)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here(in below cell), we have build a KNN model after removing the Tomek Links from SMOTE oversampled data. We have checked the predictions with \"n_neighbors\" parameter in the range 1 to 10. The \"weights\" parameter is set to \"distance\". Used n_jobs = -1 so as to make use of all the available cores for execution. The best result observed was with n_neighbors = 1 & 2.\n",
    "\n",
    "The result was like this.\n",
    "#neighbors = 1, f1_score for both classes = [0.99960245 0.97974727]\n",
    "\n",
    "#neighbors = 2, f1_score for both classes = [0.99960245 0.97974727]\n",
    "\n",
    "It can be noted that these results are in par (for both the majority and minority classes) with the predictions made over SMOTE oversampled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Nov 21 19:37:32 2018\n",
      "#neighbors = 1, f1_score for both classes = [0.99960245 0.97974727]\n",
      "#neighbors = 2, f1_score for both classes = [0.99960245 0.97974727]\n",
      "#neighbors = 3, f1_score for both classes = [0.9994732  0.97349974]\n",
      "#neighbors = 4, f1_score for both classes = [0.99949701 0.97466621]\n",
      "#neighbors = 5, f1_score for both classes = [0.99934054 0.96711864]\n",
      "#neighbors = 6, f1_score for both classes = [0.99935075 0.96758866]\n",
      "#neighbors = 7, f1_score for both classes = [0.99925548 0.96307537]\n",
      "#neighbors = 8, f1_score for both classes = [0.99925208 0.96290051]\n",
      "#neighbors = 9, f1_score for both classes = [0.9991432 0.9577323]\n",
      "#neighbors = 10, f1_score for both classes = [0.99913299 0.95726496]\n",
      "the best f1 score is [0.99960245 0.97974727]\n",
      "CPU times: user 2h 18min 21s, sys: 1min 24s, total: 2h 19min 45s\n",
      "Wall time: 1h 17min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print(time.asctime())\n",
    "\n",
    "## Fit SMOTE+TLINK train data to KNN estimator\n",
    "best_f1_knn_smotetl = [0,0]\n",
    "for N in range(1,11):\n",
    "    knn = neighbors.KNeighborsClassifier(n_neighbors=N, weights='distance', n_jobs=-1)\n",
    "    knn.fit(X_tlink, y_tlink.iloc[:,0]) \n",
    "    predictions = knn.predict(X_test)\n",
    "    f1_knn_smotetl = metrics.f1_score(y_true=y_test, y_pred=predictions, average=None)\n",
    "    knn_params = knn.get_params(deep=True)\n",
    "    print(\"#neighbors = {}, f1_score for both classes = {}\".format(N, f1_knn_smotetl))\n",
    "    if (f1_knn_smotetl[1] > best_f1_knn_smotetl[1]):\n",
    "        best_f1_knn_smotetl = f1_knn_smotetl\n",
    "print(\"the best f1 score is\", best_f1_knn_smotetl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forest classifier for training and prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our next attempt then was to try some different classifier for training and predictions. We chose RandomForest Classifier for that purpose. It is observed that RandomForest classifier is yielding better results than KNN. though we are not able give a convincing explanation for the same. We followed the same data preprocessing stages as that for KNN. Due to the same reason, there is no need for performing again the feature filtering, feature value scaling, sampling etc. The same train and validation data used by KNN estimator is used here as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here(in below cell), we have build a RandomForest Classifier model using the non sampled train data. The \"n_estimators\" is set as 50, and \"class_weight\" to \"balanced\". F1_score is predicted with mectrics \"None\".\n",
    "\n",
    "f1_score for both classes = [0.99989808 0.99475708]\n",
    "\n",
    "the confusion matrix is [[147164, 3]\n",
    " [27 , 2846]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Nov 21 20:54:43 2018\n",
      "f1_score for both classes = [0.99989808 0.99475708]\n",
      "the confusion matrix is [[147164      3]\n",
      " [    27   2846]]\n",
      "RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
      "            criterion='gini', max_depth=None, max_features='auto',\n",
      "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "            min_impurity_split=None, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=50, n_jobs=None, oob_score=False,\n",
      "            random_state=None, verbose=0, warm_start=False)\n",
      "CPU times: user 3min 37s, sys: 5.39 s, total: 3min 42s\n",
      "Wall time: 1min 17s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%%time\n",
    "\n",
    "print(time.asctime())\n",
    "\n",
    "## Fit train data to RF estimator\n",
    "rf = RandomForestClassifier(n_estimators=50,class_weight = \"balanced\")\n",
    "rf.fit(X_train, y_train)\n",
    "predictions = rf.predict(X_test)\n",
    "f1_rf = metrics.f1_score(y_true=y_test, y_pred=predictions, average=None)\n",
    "print(\"f1_score for both classes = {}\".format(f1_rf))\n",
    "print(\"the confusion matrix is\", metrics.confusion_matrix(y_true=y_test, y_pred=predictions))\n",
    "print(rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here(in below cell), we have build a RandomForest Classifier model using the random oversampled data. The \"n_estimators\" is set as 50, and \"class_weight\" to \"balanced\". F1_score is predicted with mectrics \"None\".\n",
    "\n",
    "f1_score for both classes = [0.99996263 0.99808329]\n",
    "\n",
    "the confusion matrix is [[147165 , 2]\n",
    " [9 , 2864]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Nov 21 20:56:01 2018\n",
      "f1_score for both classes = [0.99996263 0.99808329]\n",
      "the confusion matrix is [[147165      2]\n",
      " [     9   2864]]\n",
      "RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
      "            criterion='gini', max_depth=None, max_features='auto',\n",
      "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "            min_impurity_split=None, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=50, n_jobs=None, oob_score=False,\n",
      "            random_state=None, verbose=0, warm_start=False)\n",
      "CPU times: user 4min 57s, sys: 5.99 s, total: 5min 3s\n",
      "Wall time: 2min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print(time.asctime())\n",
    "\n",
    "## Fit ROS train data to RF estimator\n",
    "rf = RandomForestClassifier(n_estimators=50,class_weight = \"balanced\")\n",
    "rf.fit(X_ros, y_ros.iloc[:,0])\n",
    "predictions = rf.predict(X_test)\n",
    "f1_rfros = metrics.f1_score(y_true=y_test, y_pred=predictions, average=None)\n",
    "print(\"f1_score for both classes = {}\".format(f1_rfros))\n",
    "print(\"the confusion matrix is\", metrics.confusion_matrix(y_true=y_test, y_pred=predictions))\n",
    "print(rf)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here(in below cell), we have build a RandomForest Classifier model using the smote oversampled data. The \"n_estimators\" is set as 50, and \"class_weight\" to \"balanced\". F1_score is predicted with mectrics \"None\".\n",
    "\n",
    "f1_score for both classes = [0.99995923 0.99790941]\n",
    "\n",
    "the confusion matrix is [[147164 , 3]\n",
    " [9 , 2864]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Nov 21 20:58:26 2018\n",
      "f1_score for both classes = [0.99995923 0.99790941]\n",
      "the confusion matrix is [[147164      3]\n",
      " [     9   2864]]\n",
      "RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
      "            criterion='gini', max_depth=None, max_features='auto',\n",
      "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "            min_impurity_split=None, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=50, n_jobs=None, oob_score=False,\n",
      "            random_state=None, verbose=0, warm_start=False)\n",
      "CPU times: user 5min 24s, sys: 5.84 s, total: 5min 30s\n",
      "Wall time: 2min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print(time.asctime())\n",
    "\n",
    "## Fit SMOTE train data to RF estimator\n",
    "rf = RandomForestClassifier(n_estimators=50,class_weight = \"balanced\")\n",
    "rf.fit(X_smote, y_smote.iloc[:,0])\n",
    "predictions = rf.predict(X_test)\n",
    "f1_rfsmote = metrics.f1_score(y_true=y_test, y_pred=predictions, average=None)\n",
    "print(\"f1_score for both classes = {}\".format(f1_rfsmote))\n",
    "print(\"the confusion matrix is\", metrics.confusion_matrix(y_true=y_test, y_pred=predictions))\n",
    "print(rf) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here(in below cell), we have build a RandomForest Classifier model using the random undersampled data. The \"n_estimators\" is set as 50, and \"class_weight\" to \"balanced\". F1_score is predicted with mectrics \"None\".\n",
    "\n",
    "f1_score for both classes = [0.99979613 0.98962297]\n",
    "\n",
    "the confusion matrix is [[147119 , 48]\n",
    " [12 , 2861]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Nov 21 21:01:25 2018\n",
      "f1_score for both classes = [0.99979613 0.98962297]\n",
      "the confusion matrix is [[147119     48]\n",
      " [    12   2861]]\n",
      "RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
      "            criterion='gini', max_depth=None, max_features='auto',\n",
      "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "            min_impurity_split=None, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=50, n_jobs=None, oob_score=False,\n",
      "            random_state=None, verbose=0, warm_start=False)\n",
      "CPU times: user 13.6 s, sys: 207 ms, total: 13.8 s\n",
      "Wall time: 7.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print(time.asctime())\n",
    "\n",
    "## Fit RUS train data to RF estimator\n",
    "rf = RandomForestClassifier(n_estimators=50,class_weight = \"balanced\")\n",
    "rf.fit(X_rus, y_rus.iloc[:,0])\n",
    "predictions = rf.predict(X_test)\n",
    "f1_rfrus = metrics.f1_score(y_true=y_test, y_pred=predictions, average=None)\n",
    "print(\"f1_score for both classes = {}\".format(f1_rfrus))\n",
    "print(\"the confusion matrix is\", metrics.confusion_matrix(y_true=y_test, y_pred=predictions))\n",
    "print(rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here(in below cell), we have build a RandomForest Classifier model using the smote+tlink sampled data. The \"n_estimators\" is set as 50, and \"class_weight\" to \"balanced\". F1_score is predicted with mectrics \"None\".\n",
    "\n",
    "f1_score for both classes = [0.99994224 0.99703781]\n",
    "\n",
    "the confusion matrix is [[147162 , 5]\n",
    " [12 , 2861]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Nov 21 21:01:32 2018\n",
      "f1_score for both classes = [0.99994224 0.99703781]\n",
      "the confusion matrix is [[147162      5]\n",
      " [    12   2861]]\n",
      "RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
      "            criterion='gini', max_depth=None, max_features='auto',\n",
      "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "            min_impurity_split=None, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=50, n_jobs=None, oob_score=False,\n",
      "            random_state=None, verbose=0, warm_start=False)\n",
      "CPU times: user 5min 19s, sys: 6.34 s, total: 5min 25s\n",
      "Wall time: 2min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print(time.asctime())\n",
    "\n",
    "## Fit SMOTE+TLINK train data to RF estimator\n",
    "rf = RandomForestClassifier(n_estimators=50,class_weight = \"balanced\")\n",
    "rf.fit(X_tlink, y_tlink.iloc[:,0])\n",
    "predictions = rf.predict(X_test)\n",
    "f1_rfsmotetl = metrics.f1_score(y_true=y_test, y_pred=predictions, average=None)\n",
    "print(\"f1_score for both classes = {}\".format(f1_rfsmotetl))\n",
    "print(\"the confusion matrix is\", metrics.confusion_matrix(y_true=y_test, y_pred=predictions))\n",
    "print(rf) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting F1-score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Made an attempt to plot the f1_score of different trials above against the classifiers/algorithms used for the test. \n",
    "The x-axis shows the different algorithms and y-axis shows the f1_score of minority class.\n",
    "It can be observed that the best f1_score is obtained when RandomForest Classifier with SMOTE oversampled data.\n",
    "When using K-NearestNeighbor Classifier, the best results are obtained without using any sampling technique (random oversampling and no sampling yielded almost the same results)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fd96da7ac18>]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline \n",
    "import matplotlib.pyplot as pl\n",
    "\n",
    "# Plot histogram\n",
    "x_data = [\"rf\", \"rfrus\", \"rfros\", \"rfsmote\", \"knn\", \"knnrus\", \"knnros\", \"knnsmote\"]\n",
    "y_data = [f1_rf[1], f1_rfrus[1], f1_rfros[1], f1_rfsmote[1], best_f1_knn[1], best_f1_knn_rus[1], best_f1_knn_ros[1], best_f1_knn_smote[1]]\n",
    "pl.xlabel(\"Algorithms\")\n",
    "pl.ylabel(\"Minority class F1-score\")\n",
    "pl.plot(x_data, y_data, 'x')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GridSearch for RandomForest Classifier (an experiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an attempt to use the GridSearch to find the best estimator (with best parameters) for RandomForest Classifier.\n",
    "The unsampled data is the input to program.\n",
    "Grid search is done with following random forest parameters.\n",
    "parameters = {'class_weight':['balanced'],\n",
    "              'criterion':['gini'],\n",
    "              'max_depth': [10,20,30,40],\n",
    "              'n_estimators': [50],\n",
    "              'max_features': ['auto'],\n",
    "              'n_jobs': [-1],\n",
    "              'bootstrap': [True, False]}\n",
    "Prepared own scoring function f1_high so as to target the minority class.\n",
    "\n",
    "Here is the result with GridSearch.\n",
    "F1 (None) = [0.99995244 0.99755927]\n",
    "\n",
    "confusion_matrix = [[147165 , 2]\n",
    " [12 , 2861]]\n",
    "\n",
    "The best RF params are selected as following.\n",
    "gscv best params {'bootstrap': False, 'class_weight': 'balanced', 'criterion': 'gini', 'max_depth': 40, 'max_features': 'auto', 'n_estimators': 50, 'n_jobs': -1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Nov 21 21:04:26 2018\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "[CV] bootstrap=True, class_weight=balanced, criterion=gini, max_depth=10, max_features=auto, n_estimators=50, n_jobs=-1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  bootstrap=True, class_weight=balanced, criterion=gini, max_depth=10, max_features=auto, n_estimators=50, n_jobs=-1, total=  45.6s\n",
      "[CV] bootstrap=True, class_weight=balanced, criterion=gini, max_depth=10, max_features=auto, n_estimators=50, n_jobs=-1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   48.9s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  bootstrap=True, class_weight=balanced, criterion=gini, max_depth=10, max_features=auto, n_estimators=50, n_jobs=-1, total=  45.8s\n",
      "[CV] bootstrap=True, class_weight=balanced, criterion=gini, max_depth=10, max_features=auto, n_estimators=50, n_jobs=-1 \n",
      "[CV]  bootstrap=True, class_weight=balanced, criterion=gini, max_depth=10, max_features=auto, n_estimators=50, n_jobs=-1, total=  46.6s\n",
      "[CV] bootstrap=True, class_weight=balanced, criterion=gini, max_depth=10, max_features=auto, n_estimators=50, n_jobs=-1 \n",
      "[CV]  bootstrap=True, class_weight=balanced, criterion=gini, max_depth=10, max_features=auto, n_estimators=50, n_jobs=-1, total=  45.4s\n",
      "[CV] bootstrap=True, class_weight=balanced, criterion=gini, max_depth=10, max_features=auto, n_estimators=50, n_jobs=-1 \n",
      "[CV]  bootstrap=True, class_weight=balanced, criterion=gini, max_depth=10, max_features=auto, n_estimators=50, n_jobs=-1, total=  47.3s\n",
      "[CV] bootstrap=True, class_weight=balanced, criterion=gini, max_depth=20, max_features=auto, n_estimators=50, n_jobs=-1 \n",
      "[CV]  bootstrap=True, class_weight=balanced, criterion=gini, max_depth=20, max_features=auto, n_estimators=50, n_jobs=-1, total=  48.5s\n",
      "[CV] bootstrap=True, class_weight=balanced, criterion=gini, max_depth=20, max_features=auto, n_estimators=50, n_jobs=-1 \n",
      "[CV]  bootstrap=True, class_weight=balanced, criterion=gini, max_depth=20, max_features=auto, n_estimators=50, n_jobs=-1, total=  50.0s\n",
      "[CV] bootstrap=True, class_weight=balanced, criterion=gini, max_depth=20, max_features=auto, n_estimators=50, n_jobs=-1 \n",
      "[CV]  bootstrap=True, class_weight=balanced, criterion=gini, max_depth=20, max_features=auto, n_estimators=50, n_jobs=-1, total=  49.8s\n",
      "[CV] bootstrap=True, class_weight=balanced, criterion=gini, max_depth=20, max_features=auto, n_estimators=50, n_jobs=-1 \n",
      "[CV]  bootstrap=True, class_weight=balanced, criterion=gini, max_depth=20, max_features=auto, n_estimators=50, n_jobs=-1, total=  53.2s\n",
      "[CV] bootstrap=True, class_weight=balanced, criterion=gini, max_depth=20, max_features=auto, n_estimators=50, n_jobs=-1 \n",
      "[CV]  bootstrap=True, class_weight=balanced, criterion=gini, max_depth=20, max_features=auto, n_estimators=50, n_jobs=-1, total=  50.1s\n",
      "[CV] bootstrap=True, class_weight=balanced, criterion=gini, max_depth=30, max_features=auto, n_estimators=50, n_jobs=-1 \n",
      "[CV]  bootstrap=True, class_weight=balanced, criterion=gini, max_depth=30, max_features=auto, n_estimators=50, n_jobs=-1, total=  48.3s\n",
      "[CV] bootstrap=True, class_weight=balanced, criterion=gini, max_depth=30, max_features=auto, n_estimators=50, n_jobs=-1 \n",
      "[CV]  bootstrap=True, class_weight=balanced, criterion=gini, max_depth=30, max_features=auto, n_estimators=50, n_jobs=-1, total=  49.9s\n",
      "[CV] bootstrap=True, class_weight=balanced, criterion=gini, max_depth=30, max_features=auto, n_estimators=50, n_jobs=-1 \n",
      "[CV]  bootstrap=True, class_weight=balanced, criterion=gini, max_depth=30, max_features=auto, n_estimators=50, n_jobs=-1, total=  48.8s\n",
      "[CV] bootstrap=True, class_weight=balanced, criterion=gini, max_depth=30, max_features=auto, n_estimators=50, n_jobs=-1 \n",
      "[CV]  bootstrap=True, class_weight=balanced, criterion=gini, max_depth=30, max_features=auto, n_estimators=50, n_jobs=-1, total=  48.0s\n",
      "[CV] bootstrap=True, class_weight=balanced, criterion=gini, max_depth=30, max_features=auto, n_estimators=50, n_jobs=-1 \n",
      "[CV]  bootstrap=True, class_weight=balanced, criterion=gini, max_depth=30, max_features=auto, n_estimators=50, n_jobs=-1, total=  51.5s\n",
      "[CV] bootstrap=True, class_weight=balanced, criterion=gini, max_depth=40, max_features=auto, n_estimators=50, n_jobs=-1 \n",
      "[CV]  bootstrap=True, class_weight=balanced, criterion=gini, max_depth=40, max_features=auto, n_estimators=50, n_jobs=-1, total=  51.8s\n",
      "[CV] bootstrap=True, class_weight=balanced, criterion=gini, max_depth=40, max_features=auto, n_estimators=50, n_jobs=-1 \n",
      "[CV]  bootstrap=True, class_weight=balanced, criterion=gini, max_depth=40, max_features=auto, n_estimators=50, n_jobs=-1, total=  52.5s\n",
      "[CV] bootstrap=True, class_weight=balanced, criterion=gini, max_depth=40, max_features=auto, n_estimators=50, n_jobs=-1 \n",
      "[CV]  bootstrap=True, class_weight=balanced, criterion=gini, max_depth=40, max_features=auto, n_estimators=50, n_jobs=-1, total=  53.1s\n",
      "[CV] bootstrap=True, class_weight=balanced, criterion=gini, max_depth=40, max_features=auto, n_estimators=50, n_jobs=-1 \n",
      "[CV]  bootstrap=True, class_weight=balanced, criterion=gini, max_depth=40, max_features=auto, n_estimators=50, n_jobs=-1, total=  51.1s\n",
      "[CV] bootstrap=True, class_weight=balanced, criterion=gini, max_depth=40, max_features=auto, n_estimators=50, n_jobs=-1 \n",
      "[CV]  bootstrap=True, class_weight=balanced, criterion=gini, max_depth=40, max_features=auto, n_estimators=50, n_jobs=-1, total=  49.6s\n",
      "[CV] bootstrap=False, class_weight=balanced, criterion=gini, max_depth=10, max_features=auto, n_estimators=50, n_jobs=-1 \n",
      "[CV]  bootstrap=False, class_weight=balanced, criterion=gini, max_depth=10, max_features=auto, n_estimators=50, n_jobs=-1, total= 1.2min\n",
      "[CV] bootstrap=False, class_weight=balanced, criterion=gini, max_depth=10, max_features=auto, n_estimators=50, n_jobs=-1 \n",
      "[CV]  bootstrap=False, class_weight=balanced, criterion=gini, max_depth=10, max_features=auto, n_estimators=50, n_jobs=-1, total= 1.2min\n",
      "[CV] bootstrap=False, class_weight=balanced, criterion=gini, max_depth=10, max_features=auto, n_estimators=50, n_jobs=-1 \n",
      "[CV]  bootstrap=False, class_weight=balanced, criterion=gini, max_depth=10, max_features=auto, n_estimators=50, n_jobs=-1, total= 1.2min\n",
      "[CV] bootstrap=False, class_weight=balanced, criterion=gini, max_depth=10, max_features=auto, n_estimators=50, n_jobs=-1 \n",
      "[CV]  bootstrap=False, class_weight=balanced, criterion=gini, max_depth=10, max_features=auto, n_estimators=50, n_jobs=-1, total= 1.2min\n",
      "[CV] bootstrap=False, class_weight=balanced, criterion=gini, max_depth=10, max_features=auto, n_estimators=50, n_jobs=-1 \n",
      "[CV]  bootstrap=False, class_weight=balanced, criterion=gini, max_depth=10, max_features=auto, n_estimators=50, n_jobs=-1, total= 1.2min\n",
      "[CV] bootstrap=False, class_weight=balanced, criterion=gini, max_depth=20, max_features=auto, n_estimators=50, n_jobs=-1 \n",
      "[CV]  bootstrap=False, class_weight=balanced, criterion=gini, max_depth=20, max_features=auto, n_estimators=50, n_jobs=-1, total= 1.3min\n",
      "[CV] bootstrap=False, class_weight=balanced, criterion=gini, max_depth=20, max_features=auto, n_estimators=50, n_jobs=-1 \n",
      "[CV]  bootstrap=False, class_weight=balanced, criterion=gini, max_depth=20, max_features=auto, n_estimators=50, n_jobs=-1, total= 1.3min\n",
      "[CV] bootstrap=False, class_weight=balanced, criterion=gini, max_depth=20, max_features=auto, n_estimators=50, n_jobs=-1 \n",
      "[CV]  bootstrap=False, class_weight=balanced, criterion=gini, max_depth=20, max_features=auto, n_estimators=50, n_jobs=-1, total= 1.3min\n",
      "[CV] bootstrap=False, class_weight=balanced, criterion=gini, max_depth=20, max_features=auto, n_estimators=50, n_jobs=-1 \n",
      "[CV]  bootstrap=False, class_weight=balanced, criterion=gini, max_depth=20, max_features=auto, n_estimators=50, n_jobs=-1, total= 1.3min\n",
      "[CV] bootstrap=False, class_weight=balanced, criterion=gini, max_depth=20, max_features=auto, n_estimators=50, n_jobs=-1 \n",
      "[CV]  bootstrap=False, class_weight=balanced, criterion=gini, max_depth=20, max_features=auto, n_estimators=50, n_jobs=-1, total= 1.2min\n",
      "[CV] bootstrap=False, class_weight=balanced, criterion=gini, max_depth=30, max_features=auto, n_estimators=50, n_jobs=-1 \n",
      "[CV]  bootstrap=False, class_weight=balanced, criterion=gini, max_depth=30, max_features=auto, n_estimators=50, n_jobs=-1, total= 1.3min\n",
      "[CV] bootstrap=False, class_weight=balanced, criterion=gini, max_depth=30, max_features=auto, n_estimators=50, n_jobs=-1 \n",
      "[CV]  bootstrap=False, class_weight=balanced, criterion=gini, max_depth=30, max_features=auto, n_estimators=50, n_jobs=-1, total= 1.4min\n",
      "[CV] bootstrap=False, class_weight=balanced, criterion=gini, max_depth=30, max_features=auto, n_estimators=50, n_jobs=-1 \n",
      "[CV]  bootstrap=False, class_weight=balanced, criterion=gini, max_depth=30, max_features=auto, n_estimators=50, n_jobs=-1, total= 1.4min\n",
      "[CV] bootstrap=False, class_weight=balanced, criterion=gini, max_depth=30, max_features=auto, n_estimators=50, n_jobs=-1 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  bootstrap=False, class_weight=balanced, criterion=gini, max_depth=30, max_features=auto, n_estimators=50, n_jobs=-1, total= 1.3min\n",
      "[CV] bootstrap=False, class_weight=balanced, criterion=gini, max_depth=30, max_features=auto, n_estimators=50, n_jobs=-1 \n",
      "[CV]  bootstrap=False, class_weight=balanced, criterion=gini, max_depth=30, max_features=auto, n_estimators=50, n_jobs=-1, total= 1.3min\n",
      "[CV] bootstrap=False, class_weight=balanced, criterion=gini, max_depth=40, max_features=auto, n_estimators=50, n_jobs=-1 \n",
      "[CV]  bootstrap=False, class_weight=balanced, criterion=gini, max_depth=40, max_features=auto, n_estimators=50, n_jobs=-1, total= 1.3min\n",
      "[CV] bootstrap=False, class_weight=balanced, criterion=gini, max_depth=40, max_features=auto, n_estimators=50, n_jobs=-1 \n",
      "[CV]  bootstrap=False, class_weight=balanced, criterion=gini, max_depth=40, max_features=auto, n_estimators=50, n_jobs=-1, total= 1.3min\n",
      "[CV] bootstrap=False, class_weight=balanced, criterion=gini, max_depth=40, max_features=auto, n_estimators=50, n_jobs=-1 \n",
      "[CV]  bootstrap=False, class_weight=balanced, criterion=gini, max_depth=40, max_features=auto, n_estimators=50, n_jobs=-1, total= 1.3min\n",
      "[CV] bootstrap=False, class_weight=balanced, criterion=gini, max_depth=40, max_features=auto, n_estimators=50, n_jobs=-1 \n",
      "[CV]  bootstrap=False, class_weight=balanced, criterion=gini, max_depth=40, max_features=auto, n_estimators=50, n_jobs=-1, total= 1.3min\n",
      "[CV] bootstrap=False, class_weight=balanced, criterion=gini, max_depth=40, max_features=auto, n_estimators=50, n_jobs=-1 \n",
      "[CV]  bootstrap=False, class_weight=balanced, criterion=gini, max_depth=40, max_features=auto, n_estimators=50, n_jobs=-1, total= 1.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  40 out of  40 | elapsed: 44.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " GSCV object dumped to pickle\n",
      "\n",
      " gscv best score 0.9962900490496053\n",
      "\n",
      " gscv best params {'bootstrap': False, 'class_weight': 'balanced', 'criterion': 'gini', 'max_depth': 30, 'max_features': 'auto', 'n_estimators': 50, 'n_jobs': -1}\n",
      "\n",
      " gscv best index 6\n",
      "\n",
      " gscv best estimator RandomForestClassifier(bootstrap=False, class_weight='balanced',\n",
      "            criterion='gini', max_depth=30, max_features='auto',\n",
      "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "            min_impurity_split=None, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=50, n_jobs=-1, oob_score=False, random_state=None,\n",
      "            verbose=0, warm_start=False)\n",
      "f1_score for both classes = [0.99994904 0.99738448]\n",
      "the confusion matrix is [[147165      2]\n",
      " [    13   2860]]\n",
      "\n",
      "\n",
      "Precision (micro) = 0.9999000266595575\n",
      "\n",
      "\n",
      "Recall (micro) = 0.9999000266595575\n",
      "\n",
      "\n",
      "F1 (micro) = 0.9999000266595575\n",
      "\n",
      "\n",
      "Precision (macro) = 0.9996064297808919\n",
      "\n",
      "\n",
      "Recall (macro) = 0.9977307615594891\n",
      "\n",
      "\n",
      "F1 (macro) = 0.9986667603239993\n",
      "\n",
      "\n",
      "Precision (weighted) = 0.9998999819026951\n",
      "\n",
      "\n",
      "Recall (weighted) = 0.9999000266595575\n",
      "\n",
      "\n",
      "F1 (weighted) = 0.999899932650828\n",
      "\n",
      "\n",
      "Precision (None) = [0.99991167 0.99930119]\n",
      "\n",
      "\n",
      "Recall (None) = [0.99998641 0.99547511]\n",
      "\n",
      "\n",
      "F1 (None) = [0.99994904 0.99738448]\n",
      "\n",
      "\n",
      "confusion_matrix = [[147165      2]\n",
      " [    13   2860]]\n",
      "CPU times: user 2h 42min 12s, sys: 5min 48s, total: 2h 48min\n",
      "Wall time: 44min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print(time.asctime())\n",
    "\n",
    "## Grid Search to find the best parameters for RF\n",
    "\n",
    "parameters = {'class_weight':['balanced'],\n",
    "              'criterion':['gini'],\n",
    "              'max_depth': [10,20,30,40],\n",
    "              'n_estimators': [50],\n",
    "              'max_features': ['auto'],\n",
    "              'n_jobs': [-1],\n",
    "              'bootstrap': [True, False]}\n",
    "f1_high = metrics.make_scorer(metrics.f1_score, pos_label = 1, average = \"binary\")\n",
    "\n",
    "gscv = GridSearchCV(RandomForestClassifier(), parameters, cv = 5, verbose=2, scoring = f1_high)\n",
    "gscv.fit(X_train, y_train)\n",
    "\n",
    "pickle.dump(file=open(\"gscv_model\" + \".pkl\", \"wb\"), obj=gscv)\n",
    "print(\"\\n GSCV object dumped to pickle\")\n",
    "\n",
    "print(\"\\n gscv best score\", gscv.best_score_)\n",
    "print(\"\\n gscv best params\", gscv.best_params_ )\n",
    "print(\"\\n gscv best index\", gscv.best_index_)\n",
    "print(\"\\n gscv best estimator\", gscv.best_estimator_)\n",
    "\n",
    "best_estimator = gscv.best_estimator_\n",
    "predictions = []\n",
    "predictions.append(best_estimator.predict(X_test))\n",
    "testresult = np.array(predictions).sum(axis=0)\n",
    "testresult = [0 if item == 0 else 1 for item in testresult]\n",
    "testresult = pd.DataFrame(testresult)\n",
    "testresult = testresult.iloc[:,0]\n",
    "print(\"f1_score for both classes = {}\".format(metrics.f1_score(y_true=y_test, y_pred=testresult, average=None)))\n",
    "print(\"the confusion matrix is\", metrics.confusion_matrix(y_true=y_test, y_pred=testresult))\n",
    "\n",
    "avg_vals = [\"micro\", \"macro\", \"weighted\", None]\n",
    "\n",
    "with open (\"gscv_results.txt\", \"a\") as cv:\n",
    "    \n",
    "    cv.write (\"\\n------------------best score----------------------------\\n\")\n",
    "    cv.write (str (gscv.best_score_) + \"\\n\")\n",
    "    cv.write (\"\\n------------------best params---------------------------\\n\")\n",
    "    cv.write (str (gscv.best_params_) + \"\\n\")\n",
    "    cv.write (\"\\n------------------best index of cv results--------------\\n\")\n",
    "    cv.write (str (gscv.best_index_ ) + \"\\n\")\n",
    "    cv.write (\"\\n------------------best estimator------------------------\\n\")\n",
    "    cv.write (str (gscv.best_estimator_) + \"\\n\")\n",
    "    \n",
    "    cv.write (\"------------------Scores----------------------------------\\n\")\n",
    "    for avg_val in avg_vals:\n",
    "        str_val = (\"\\n\\nPrecision (\" \n",
    "                   + str(avg_val) \n",
    "                   + \") = \" \n",
    "                   + str(metrics.precision_score(y_true=y_test, y_pred=testresult, average=avg_val)))\n",
    "\n",
    "        print (str_val)\n",
    "        cv.write (str_val)\n",
    "        \n",
    "        str_val = (\"\\n\\nRecall (\" \n",
    "                   + str(avg_val) \n",
    "                   + \") = \" \n",
    "                   + str(metrics.recall_score(y_true=y_test, y_pred=testresult, average=avg_val)))\n",
    "\n",
    "        print (str_val)\n",
    "        cv.write (str_val)\n",
    "        \n",
    "        str_val = (\"\\n\\nF1 (\" \n",
    "                   + str(avg_val) \n",
    "                   + \") = \" \n",
    "                   + str(metrics.f1_score(y_true=y_test, y_pred=testresult, average=avg_val)))\n",
    "\n",
    "        print (str_val)\n",
    "        cv.write (str_val)\n",
    "        \n",
    "    str_val = (\"\\n\\nconfusion_matrix\"\n",
    "                   + \" = \" \n",
    "                   + str(metrics.confusion_matrix(y_true=y_test, y_pred=testresult)))\n",
    "    print (str_val)\n",
    "    cv.write (str_val)\n",
    "    \n",
    "    cv.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The best estimator for submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plots made earlier (with all the test results), makes it quite evident that we have got the best results with random forest classifier over the random and SMOTE oversampled data. Let us redo the training and validation once again(we selected the random oversampled data), and save the model for later testing. We used random_state parameter as 10 for consistency in behaviour when re-executing the code.\n",
    "\n",
    "The model is saved in a pickle file \"rf_model.pkl\" which an can be used in testing phase. Refer testing instructions in the final cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Nov 22 09:39:57 2018\n",
      "f1_score for both classes = [0.99995923 0.99790868]\n",
      "the confusion matrix is [[147165      2]\n",
      " [    10   2863]]\n",
      "RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
      "            criterion='gini', max_depth=None, max_features='auto',\n",
      "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "            min_impurity_split=None, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=50, n_jobs=None, oob_score=False, random_state=10,\n",
      "            verbose=0, warm_start=False)\n",
      "\n",
      " RFECV object dumped to pickle\n",
      "\n",
      " the shape of final training data (782929, 63)\n",
      "CPU times: user 4min 48s, sys: 6.11 s, total: 4min 54s\n",
      "Wall time: 2min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print(time.asctime())\n",
    "\n",
    "## Fit SMOTE train data to RF estimator\n",
    "X = pd.read_pickle(\"rostraininginputpickle\")\n",
    "y = pd.read_pickle(\"rostrainingoutputpickle\")\n",
    "rf = RandomForestClassifier(n_estimators=50,class_weight = \"balanced\", random_state = 10)\n",
    "rf.fit(X, y.iloc[:,0])\n",
    "predictions = rf.predict(X_test)\n",
    "f1_rfros = metrics.f1_score(y_true=y_test, y_pred=predictions, average=None)\n",
    "print(\"f1_score for both classes = {}\".format(f1_rfros))\n",
    "print(\"the confusion matrix is\", metrics.confusion_matrix(y_true=y_test, y_pred=predictions))\n",
    "print(rf)\n",
    "\n",
    "pickle.dump(file=open(\"rf_model\" + \".pkl\", \"wb\"), obj=rf)\n",
    "print(\"\\n RFECV object dumped to pickle\")\n",
    "\n",
    "print(\"\\n the shape of final training data\", X.shape)\n",
    "with open('final_trained_features.pkl', 'wb') as f:\n",
    "    pickle.dump(X.columns, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning\n",
    "This code is now commented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "##del X_train\n",
    "##del y_train\n",
    "##del X_test\n",
    "##del y_test\n",
    "##del train_df\n",
    "##del anomalies_df\n",
    "##del X_ros\n",
    "##del y_ros\n",
    "##del X_rus\n",
    "##del y_rus\n",
    "##del X_smote\n",
    "##del y_smote\n",
    "##del X_tlink\n",
    "##del y_tlink"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform the following steps:\n",
    "\n",
    "Note: when you are running this program the path of source files are to be appropriately set(cell #3).\n",
    "\n",
    "1. Read the test data into a python dataframe.\n",
    "\n",
    "2. Extract the \"isAnomaly\" feature from the test data (if not already seperated), and replace \"True\" and \"False\" with 1 \n",
    "   and 0 respectively (reference code in Cell #5 and Cell #6).\n",
    "\n",
    "3. Execute the same procedure that we have followed and generate the same dataframe from the originally given training \n",
    "   data (have to perform feature filtering (finally only 63 features exist, then do MinMaxScaling, then do SMOTE \n",
    "   oversampling and generate the dataframe)). Reference code for feature filtering is in Cells #7, #8 and #9. The \n",
    "   reference code for MinMax scaling in cell #10. The refence code for SMOTE oversampling is in cell #13.\n",
    "   \n",
    "4. Keep the same feature set in test data(i.e., only 63 features should be there, remove the rest from test data too). \n",
    "   The finally trained feature set is also captured in \"final_trained_features.pkl\" for your reference.\n",
    "\n",
    "4. Load the best estimator in rf_model.pkl(Refer cell #28). Or else you can build the model as we have built here in Cell \n",
    "   #28.\n",
    "\n",
    "5. Predict the result using test data on this model.\n",
    "\n",
    "Additional note: If you are running this whole program (the attached .ipynb program), it may be better to modify the \n",
    "KNN n_neighbors iterator to 1 (now it runs for 10 iterations) as otherwise it would take much time to finish the \n",
    "execution(also best results with KNN is always there with n_ neighbors = 1 and 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
